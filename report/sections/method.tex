\subsection{Data retrieval}
A crawler was implemented to extract articles from BBC's RSS feed\footnote{\url{http://www.bbc.com/news/10628494}}. The crawler extracts the content of each article from eight different topics, namely
\begin{itemize}[noitemsep,nolistsep]
	\item Business
	\item Education
	\item Entertainment and Art
	\item Health
	\item Politics
	\item Science and Environment
	\item Sports
	\item Technology.
\end{itemize}
A total of 693 articles were extracted. As training set 2/3 of the articles were randomly selected and the remaining 1/3 were used as test set. Notable is that the distribution of topics is not uniform since the RSS feeds of the topics are not updated equally frequent. 
\subsection{Coding}
Python is chosen as programming language in order to extend our knowledge and experience of implementing different types of classifiers using the Scikit-learn machine learning library in python and further understand how the data has to be prepared and presented to the classifiers.
\subsection{Preparation of Data}
To classify the articles as accurately and fast as possible, the data is prepared in such way that it contains as much information as possible in a format as dense as possible. To achieve this, the articles are parsed into words with white-space as separator, then removed of all characters not found in the English alphabet, though keeping the '-' character. The words are stemmed so that similar words would be on the same form, and not be seen as two different words. (E.g. "argued", "argues", "arguing" are reduced to "argu".) After stemming, all so called stop words are removed (such as "a", "the", etc.). 
\subsection{Vocabulary}
The vocabulary is built up by unique stemmed words from the training set which consists of 66\% of all articles. By different pruning techniques the vocabulary is reduced in size and only the most informative elements are kept. For the feature selection the score is determined by the $\chi^2$ function. The vocabulary is then pruned by selecting a percentile of the best features.
\subsection{Datatypes}
There are many different ways to represent a feature vector, and different classifiers might want it represented in different ways. We have chosen to investigate a couple of different types of representations of the vocabulary. The different datatypes were
\begin{description}[noitemsep,nolistsep]
\item[Binary:]\ \\Does article contain word or not.
\item[Count:]\ \\How many times does article contain word.

%%%%%%%%%%%%%% DRAFT PART %%%%%%%%%%%%%%%%%%%%
\ifnum\printdraft>0
	% Bör denna vara med eller ej? (Vi kanske kan skriva den som en misslyckad data-typ?)
	\item[Normalized Count by article length:]\ \\ The count array divided by the number of words in article (sum $<$ 1).
\else
\begin{center}
	\textbf{--- CORRECTION PARTS ---}
\end{center}
\fi
%%%%%%%%%%%%%%%%%%%% END DRAFT PART %%%%%%%%%%%%%%%%%%%%%%

\item[$L_2$-normalized:]\ \\ The count array normalized with the $L_2$ norm. 
\item[Mapped value from 0 to 1:]\ \\ $\frac{c_i}{\max(c)}$ where $c_i$ is the count of word $i$.
\end{description}

\subsection{Classifiers}
In the Scikit-learn package\footnote{\url{http://scikit-learn.org/}} there are plenty of different classifiers, amongst them the four we want to investigate. The four classifiers we will be investigating are
\begin{itemize}[noitemsep,nolistsep]
\item Naïve Bayes: Bernoulli
\item Naïve Bayes: Multinomial
\item Support Vector machines (SVM)
\item Random forest 
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%% DRAFT PART %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\ifnum\printdraft>0
\subsection{TODO}
\begin{itemize}
\item Divided into 2/3 training set, 1/3 test set
\end{itemize}
\else
\begin{center}
\textbf{--- DRAFT PARTS ---}
\end{center}
\fi
%%%%%%%%%%%%%%%%%%%%% END OF DRAFT PART %%%%%%%%%%%%%%%%%%%%%%%%%