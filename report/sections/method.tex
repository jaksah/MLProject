\subsection{News Crawler}
A web crawler extracts articles from BBC\footnote{\url{http://www.bbc.com/}}, and the topic that the article belongs to, which is written in the HTML-code of the page on BBC's articles. In total, about \texttt{--NUMBER--} articles is extracted under \texttt{--NUMBER--} different topics. This is used as training and test data for the classifiers.
\subsection{Coding}
Python is chosen as programming language in order to extend our knowledge and experience of implementing different types of classifiers using the \texttt{scikit-learn} machine learning library in python and further understand how the data has to be prepared and presented to the classifiers.
\subsection{Preparation of Data}
To classify the articles as accurately and fast as possible, the data is prepared in such way that it contains as much information as possible in a format as dense as possible. To achieve this, the articles are parsed into words with white-space as separator, then removed of all characters not found in the English alphabet, though keeping the '-' character. The words are stemmed so that similar words would be on the same form, and not be seen as two different words. (E.g. "argued", "argues", "arguing" are reduced to "argu".) After stemming, all so called stop words are removed (such as "a", "the", etc.). 
\subsection{Vocabulary}
The vocabulary is built up by unique stemmed words from the training set which consists of 66\% of all articles. By different pruning techniques the vocabulary is reduced in size and only the most informative elements are kept. For the feature selection the score is determined by the $\chi^2$ function. The vocabulary is then pruned by selecting a percentile of the best features.
\subsection{Datatypes}
There are various ways to represent a feature vector, and different classifiers might require different representations. Three different types of representations of the feature vectors are investigated, namely
\begin{description}[noitemsep,nolistsep]
\item[Binary:]\ \\Does article contain word or not.
\item[Count:]\ \\How many times does article contain word.
\item[Normalized Count by L2 norm:]\ \\ The count array normalized by the L2 norm.
\end{description}

\subsection{Classifiers}
In the Scikit-learn package\footnote{\url{http://scikit-learn.org/}} there are plenty of different classifiers, amongst them the four we want to investigate. The four classifiers we will be investigating are
\begin{itemize}[noitemsep,nolistsep]
\item Naïve Bayes: Bernoulli
\item Naïve Bayes: Multinomial
\item Support Vector machines (SVM)
\item Random forest 
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%% DRAFT PART %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\ifnum\printdraft>0
\subsection{TODO}
\begin{itemize}
\item Started with Naïve Bayes Classifier
\item Downloaded about 200 articles to classify
\item Divided into 2/3 training set, 1/3 test set
\item Parsed the words, removed non-alphabetical letters, kept -
\item Stemmed the words in the array
\item Removed empty articles
\item Removed stop words (such as "a","the" and so on.)
\item Used information gain to sample a good feature vector smaller than the one used earlier.
\item Describe the different in-datatypes.
\end{itemize}
\fi
%%%%%%%%%%%%%%%%%%%%% END OF DRAFT PART %%%%%%%%%%%%%%%%%%%%%%%%%