Interesting results and aspects of the report will be discussed further under this section, where we will give our analysis and input on \emph{why} we think the results turned out how they did. Later, under Section \ref{sec:conc_future}, conclusions will be drawn, along with suggestions on what future projects should focus upon.

\subsection{TEMP: SVM handles increase in vocabulary size well} % (fold)
In Figure \ref{fig:confmat-svm} we see that \svm\ is resistant to increasing number of features. This follows the properties of the classifier mentioned in Section \ref{sec:theory_svm} saying that \svm s are suitable for problems with high dimensionality.
% subsection temp_svm_handles_increase_in_vocabulary_size_well (end)

\subsection{TEMP: Why is Random Forest robust to data type?} % (fold)
\label{sub:temp_why_is_random_forest_robust_to_data_type_}
The \rf\ classifier splits the data into two parts in each node depending on threshold values, so if the data is normalized or not, when building the tree, does not really matter for the classifier. We expected to see more difference in the binary data type, due to the fact that it discards how \emph{many} of each word the article contains.
% subsection temp_why_is_random_forest_robust_to_data_type_ (end)

\subsection{TEMP: Random Forest needs balanced data} % (fold)
\label{sub:temp_random_forest_needs_balanced_data}
One of the reason to why the \rf\ classifier tends to classify too many as Sports is that \rf\ is sensitive to imbalanced training data, i.e., not having the same amount of training data for each class. In our case, we have more articles of the class Sports, and less of Politics and Health. The classification in Figure \ref{fig:confmat} follows this reasoning and shows that Sports was classified 50 \% more than there are Sports articles, while Politics and Health were classified 23 \% respectively 42 \% less than there are articles of those classes.
% subsection temp_random_forest_need_balanced_data (end)

\subsection{TEMP: Hybrid follows Multinomial due to default} % (fold)
\label{sub:temp_hybrid_follows_multinomial_due_to_default}
In Figure \ref{fig:hitratio} \& \ref{fig:confmat} we can see that the \hy\ classifier follows the trend of \mn, which is due to the fact that \mn\ is set as default in case of a tie in the voting.
% subsection temp_hybrid_follows_multinomial_due_to_default (end)

\subsection{Feature Selection}
The feature selection used is based on a statistical approach; the features that are statistically dependent of the classes are rewarded a high score and are therefore kept. This is reasonable since the features that are independent of the classes does not contribute with information to the classification. It does not, however, take feature interaction into account. It might be the case that a group of features with lower scores might give more information than a feature with a high score. 

\subsection{Hit ratio vs number of articles in training set}
As illustrated in Figure \ref{fig:hitratio-data}, the hit ratio as a function of articles in the training set has a similar behavior for all the classifiers. In general, there is an increase of the hit ratio between zero and 100 articles, and for more than 100 articles the hit ratio is constant or decreases slightly. The hit ratio peak is reached for a relatively small number of articles, considering that there are eight different article classes. This could be explained by the fact that all the articles are from the same time, and therefore cover very similar or the same events. If all articles contain very distinguishing features, these features will be found even in a small subset of the training data.

%\subsection{TEMP: Why do we only need like 50 articles to get a good classification?} % (fold)
%\label{sub:temp_why_do_we_only_need_like_50_articles_to_get_a_good_classification_}
%We can see in Figure \ref{fig:hitratio-data} that by after training on just 50 articles the classifiers were able to get a good hit ratio. This might be due to the fact that each article contains lots of words, which will be enough to get a reasonable hit ratio on the test set. Another reason might be that the articles are crawled very close to each other, so that articles might be about similar news, using similar words.
% subsection temp_why_do_we_only_need_like_50_articles_to_get_a_good_classification_ (end)

\subsection{TEMP: Similarities between Tech / Science and Business / Education} % (fold)
\label{sub:temp_similarities_between_tech_science_and_business_education}
As stated in Section \ref{sec:result} the topics Technology and Science \& Environment has a tendency of getting mixed up by all classifiers. This is not a big surprise since in nature these two topics are closely related, e.g., an article about a new technology that reduces the environmental footprint at a technology cooperation could legitimately be classified as either of the two topics.\\\\
Why Sports is easy to classify may depend on the fact that we attained more articles about Sports than any other topic which made the classifiers have a prior probability to predict Sports but also more data to classify it with. Sports also uses a quite unique vocabulary compared to other topics since Sports articles' target audience is those who are already interested in the topic in contrast to other topics which aims to reach as big audience as possible.
% subsection temp_similarities_between_tech_science_and_business_education (end)

\subsection{TEMP: Hard to classify for person} % (fold)
\label{sub:temp_hard_to_classify_for_person}
Even a person might have difficulties to classify an article after reading it. A reason for this might be that the particular article does not contain enough information in order to distinguish the topic, which makes the classification difficult for a person as well. Imagine for example a football player, visiting a school to talk about politics.
\\\\
If we compare recognition of handwritten letters done by humans and computers, we might find that the hit ratio will be similar. We might have lots of data for recognition of handwritten letters, but when it comes to news articles, the news changes over time. For a human, a politics article might be distinguishable from a sports article due to the fact that we probably know more or less all words, but for our classifier, this will be a harder task if it was never exposed to the words. The classifier will need a lot of data over a long time to be able to recognize many types of words from different types of news.
\\\\
It would be interesting to see how the hit ratio would turn out if a human was to classify 100 articles from these eight topics. 
% subsection temp_hard_to_classify_for_person (end)