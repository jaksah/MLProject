%%%%%%%%%%%%%%%%%%%% DRAFT %%%%%%%%%%%%%%
\ifnum\printdraft>0
	\begin{itemize}
		%\item Random forest is sensitive to uneven spread of data between topics. Probably needs same amount of each
		\item SVM handles increase in vocabulary size well. Should be resistant to high dimensionality.Feedback to theory.
		\item Why might sport and education be easy?
		%\item Hybrid follows Multinomial since it's the default.
		\item Similarities between Tech / Science and Business / Education
		%\item Low amount of data. Difficult to classify, even for a person.
		%\item Why is random forest robust to data type?
		\item Naive bayes good for prediction, bad estimator
		%\item All articles crawled at two points in time. Several articles may be about same thing, making it easier to predict with fewer articles in train set.
		%\item Why do we only need like 50 articles to get a good classification? Maybe the thing with articles crawler at the same time.
	\end{itemize}
\else
\begin{center}
	\textbf{--- DRAFT PARTS ---}
\end{center}
\fi
%%%%%%%%%%%%%%%%%%%% END DRAFT %%%%%%%%%%
Interesting results and aspects of the report will be discussed further under this section, where we will give our analysis and input on \emph{why} we think the results turned out how they did. Later, under section \ref{sec:conc_future}, conclusions will be drawn, along with suggestions on what future projects should focus upon.

\subsection{TEMP: Similarities between Tech / Science and Business / Education} % (fold)
\label{sub:temp_similarities_between_tech_science_and_business_education}
Yeeeah!
% subsection temp_similarities_between_tech_science_and_business_education (end)

\subsection{TEMP: Na√Øve Bayes good for prediction, bad estimator} % (fold)
\label{sub:temp_na_ve_bayes_good_for_prediction_bad_estimator}
Say whaaat?
% subsection temp_na_ve_bayes_good_for_prediction_bad_estimator (end)

\subsection{TEMP: Why might sport and education be easy?} % (fold)
\label{sub:temp_why_might_sport_and_education_be_easy_}
Cuz such cool.
% subsection temp_why_might_sport_and_education_be_easy_ (end)

\subsection{TEMP: SVM handles increase in vocabulary size well} % (fold)
\label{sub:temp_svm_handles_increase_in_vocabulary_size_well}
FILL DIS!
% subsection temp_svm_handles_increase_in_vocabulary_size_well (end)

\subsection{TEMP: Why do we only need like 50 articles to get a good classification?} % (fold)
\label{sub:temp_why_do_we_only_need_like_50_articles_to_get_a_good_classification_}
We can see in figure \ref{fig:hitratio-data} that by after training on just 50 articles the classifier was able to get a good hit ratio. This might be due to the fact that each article contains lots and lots of words, which will be enough to get a reasonable hit ratio on the test set. Another reason might be that the articles are crawled very close to each other, so that articles might be about similar news, using similar words.
% subsection temp_why_do_we_only_need_like_50_articles_to_get_a_good_classification_ (end)

\subsection{TEMP: Why is Random Forest robust to data type?} % (fold)
\label{sub:temp_why_is_random_forest_robust_to_data_type_}
The Random Forest classifier splits the data into two parts in each node depending on threshold values, so if the data is normalized or not, when building the tree, does not really matter for the classifier. What we would have expected would be to see a little bit difference in the binary data type, due to the fact that it discards how \emph{many} of each word the article contains.
% subsection temp_why_is_random_forest_robust_to_data_type_ (end)

\subsection{TEMP: Random Forest needs balanced data} % (fold)
\label{sub:temp_random_forest_needs_balanced_data}
One of the reason to why the Random Forest classifier tends to classify too many as sports is that Random Forest is sensitive to imbalanced training data, i.e., not having the same amount of training data for each class. In our case, we have more articles of the class sports, and less of politics and health. The classification in figure \ref{fig:confmat} follows this reasoning and shows that sports was classified 50 \% more than there are sports articles, while politics and health were classified 23 \% respectively 42 \% less than there are articles of that class.
% subsection temp_random_forest_need_balanced_data (end)

\subsection{TEMP: Hybrid follows Multinomial due to default} % (fold)
\label{sub:temp_hybrid_follows_multinomial_due_to_default}
In figures \ref{fig:hitratio} \& \ref{fig:confmat} we can see that the Hybrid classifier follows the trend of Multinomial, which is due to the fact that Multinomial is set as default in case of a tie in the voting.
% subsection temp_hybrid_follows_multinomial_due_to_default (end)

\subsection{TEMP: Hard to classify for person} % (fold)
\label{sub:temp_hard_to_classify_for_person}
Even a person might have difficulties to classify an article after reading it. A reason for this might be that the particular article does not contain enough information in order to distinguish the topic, which makes the classification difficult for a person as well. Imagine for example a football player, visiting a school to talk about politics.
\\\\
If we compare recognition of handwritten letters done by humans and computers, we might find that the hit ratio will be similar. We might have lots of data for recognition of handwritten letters, but when it comes to news articles, the news changes over time. For a human, a politics article might be distinguishable from a sports article due to the fact that we probably know more or less all words, but for our classifier, this will be a harder task if it was never exposed to the words. The classifier will need a lot of data over a long time to be able to recognize many types of words from different types of news.
\\\\
If one were to do a test out of a hundred articles from these eight topics, it would be interesting to see on average how the hit rate for a human would turn out.
% subsection temp_hard_to_classify_for_person (end)

\subsection{Hit ratio vs number of articles in training set}
As illustrated in Figure \ref{fig:hitratio-data}, the hit ratio as a function of articles in the training set has a similar behavior for all the classifiers. In general, there is an increase of the hit ratio between zero and 100 articles, and for more than 100 articles the hit ratio is constant or decreases slightly. The hit ratio peak is reached for a relatively small number of articles, considering that there are eight different article classes. This could be explained by the fact that all the articles are from the same time, and therefore cover very similar or the same events. If all articles contain very distinguishing features, these features will be found even in a small subset of the training data.
