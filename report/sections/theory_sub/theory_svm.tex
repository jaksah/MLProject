\subsection{Support Vector Machine Classifier}
Support vector machine classifiers (SVM's) classifies data belonging to two classes by finding the hyperplane with the widest margin that separates the classes. The data vectors that restrict the margin of the hyperplane are referred to as support vectors. These also fully specify the decision function. Finding the optimal hyperplane is a maximization problem, where the objective function describes the width of the margin. The problem can be formulated using Lagrangian multipliers and is then solved using quadratic programming. An advantage with this approach is that the maximization problem is convex, meaning that the maximum found is guaranteed to be the global maximum. This requires, however, that the classes are linearly separable. If they are not, one can make use of a so called kernel function that maps the data onto a space in which the classes might be linearly separable. Support vector machines are suitable for problems with high dimensionality, but might perform badly if the number of samples is much smaller than the number of dimensions.