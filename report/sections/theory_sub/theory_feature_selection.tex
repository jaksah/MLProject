\subsection{Feature selection}
Features that provide more information than others can be emphasized by feature selection. One way of assigning a score to each feature is to use the $\chi^2$ function, which is a measure of how independent a feature is of the classes. The lower the score, the more independent is the feature of the classes. Features with high scores are therefore desired. The $\chi^2$ score for a feature is given by
\[
\chi^2 = \sum_{i=1}^{N_s} \sum_{j=1}^{N_f} \frac{\left ( O_{i,j} - E_{i,j} \right )^2}{E_{i,j}},
\]
where $N_s$ is the number of samples, $N_f$ the number of features, $O_{i,j}$ is the count of feature $j$ in sample $i$, and $E_{i,j}$ is the expected count, given by
\[
E_{i,j} = \left ( \sum_{n_f=1}^{N_f} O_{i,n_f} \right ) \cdot \left ( \sum_{n_s=1}^{N_s} O_{n_s,j} \right ).
\]